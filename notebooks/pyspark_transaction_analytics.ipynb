{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7HKYLMLKzSi"
      },
      "source": [
        "# PySpark Transaction Analytics â€“ Mini Project\n",
        "\n",
        "This notebook demonstrates the use of PySpark for scalable data processing and analytics\n",
        "on a large synthetic transactional dataset.\n",
        "\n",
        "The objective is to compute distributed behavioral metrics and identify high-activity\n",
        "patterns that resemble use cases commonly found in fintech and AML environments, such as\n",
        "transaction monitoring, user behavior analysis and risk segmentation.\n",
        "\n",
        "The focus of this project is on distributed data transformations and aggregations rather\n",
        "than advanced modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLvutYubJWpx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SyntheticTransactionData\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rSuSZiP6lkE"
      },
      "source": [
        "## Data Generation\n",
        "\n",
        "Synthetic transactional data is generated using PySpark to simulate a large-scale\n",
        "fintech-like dataset. The dataset structure resembles typical transaction logs\n",
        "used in financial services.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpG1EIXYJ2bh"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import (\n",
        "    col, rand, expr, monotonically_increasing_id\n",
        ")\n",
        "\n",
        "NUM_TRANSACTIONS = 500_000\n",
        "NUM_USERS = 10_000\n",
        "\n",
        "df = spark.range(NUM_TRANSACTIONS)\n",
        "\n",
        "df = (\n",
        "    df.withColumn(\"transaction_id\", monotonically_increasing_id())\n",
        "      .withColumn(\"user_id\", (rand() * NUM_USERS).cast(\"int\"))\n",
        "      .withColumn(\"amount\", (rand() * 500).cast(\"double\"))\n",
        "      .withColumn(\"currency\", expr(\"CASE WHEN rand() < 0.8 THEN 'EUR' ELSE 'USD' END\"))\n",
        "      .withColumn(\n",
        "          \"country\",\n",
        "          expr(\"\"\"\n",
        "              CASE\n",
        "                  WHEN rand() < 0.6 THEN 'ES'\n",
        "                  WHEN rand() < 0.8 THEN 'FR'\n",
        "                  ELSE 'DE'\n",
        "              END\n",
        "          \"\"\")\n",
        "      )\n",
        "      .withColumn(\n",
        "          \"merchant_category\",\n",
        "          expr(\"\"\"\n",
        "              CASE\n",
        "                  WHEN rand() < 0.3 THEN 'groceries'\n",
        "                  WHEN rand() < 0.5 THEN 'electronics'\n",
        "                  WHEN rand() < 0.7 THEN 'travel'\n",
        "                  ELSE 'other'\n",
        "              END\n",
        "          \"\"\")\n",
        "      )\n",
        "      .withColumn(\n",
        "          \"channel\",\n",
        "          expr(\"\"\"\n",
        "              CASE\n",
        "                  WHEN rand() < 0.5 THEN 'online'\n",
        "                  ELSE 'pos'\n",
        "              END\n",
        "          \"\"\")\n",
        "      )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLQG0_xILAXM"
      },
      "source": [
        "## Data Validation\n",
        "\n",
        "The schema and a sample of records are inspected to ensure data consistency\n",
        "and correct data types before proceeding with the analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfqp-ho5J_KM",
        "outputId": "ec26b170-d40c-4b8c-e06d-672de8ec54f6"
      },
      "outputs": [],
      "source": [
        "df.printSchema()\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlPiLrvdKEeF"
      },
      "outputs": [],
      "source": [
        "df.write.mode(\"overwrite\").parquet(\"/content/transactions_parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqB2gZec7cmO"
      },
      "source": [
        "## User-Level Aggregated Metrics\n",
        "\n",
        "User-level aggregations are a common first step in transaction monitoring.\n",
        "Here we compute basic behavioral metrics per user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cc603p97gPW",
        "outputId": "cc0b71cb-a004-419e-caed-8d2e74623b6c"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count, sum, avg\n",
        "\n",
        "user_metrics = (\n",
        "    df.groupBy(\"user_id\")\n",
        "      .agg(\n",
        "          count(\"*\").alias(\"num_transactions\"),\n",
        "          sum(\"amount\").alias(\"total_amount\"),\n",
        "          avg(\"amount\").alias(\"avg_amount\")\n",
        "      )\n",
        ")\n",
        "\n",
        "user_metrics.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nCC5mfZ7jdR"
      },
      "source": [
        "## Global Distributions and Thresholds\n",
        "\n",
        "Understanding the global distribution of transaction amounts helps define\n",
        "thresholds for identifying extreme or unusual behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqQCRoSt7iVa",
        "outputId": "6f21d3a4-22bb-4b12-a646-4b3b81a372d3"
      },
      "outputs": [],
      "source": [
        "df.select(\"amount\").describe().show()\n",
        "\n",
        "p95, p99 = df.approxQuantile(\"amount\", [0.95, 0.99], 0.01)\n",
        "p95, p99\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CkT5vCC7tnh"
      },
      "source": [
        "## High-Activity Behavioral Patterns\n",
        "\n",
        "Rather than performing fraud detection, we identify high-activity patterns\n",
        "that may warrant further investigation in a real monitoring system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_e_bWnl7iY7",
        "outputId": "5a24c248-8d55-40f8-c401-1a7a9df9e588"
      },
      "outputs": [],
      "source": [
        "high_volume_users = user_metrics.filter(col(\"num_transactions\") > 100)\n",
        "high_volume_users.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tXvmbsC7xFk",
        "outputId": "339ef976-87ab-4f5d-d752-607c138e4325"
      },
      "outputs": [],
      "source": [
        "high_avg_amount_users = user_metrics.filter(col(\"avg_amount\") > p99)\n",
        "high_avg_amount_users.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8-K-cdK71zO"
      },
      "source": [
        "## Segmentation by Country and Channel\n",
        "\n",
        "Segmenting transactions by geography and channel allows comparison of\n",
        "behavioral patterns across different contexts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggDgx_N871IV",
        "outputId": "59acfe68-dd30-4b98-9b25-e40b76e2615b"
      },
      "outputs": [],
      "source": [
        "country_channel_stats = (\n",
        "    df.groupBy(\"country\", \"channel\")\n",
        "      .agg(\n",
        "          count(\"*\").alias(\"num_transactions\"),\n",
        "          avg(\"amount\").alias(\"avg_amount\")\n",
        "      )\n",
        ")\n",
        "\n",
        "country_channel_stats.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8IVA18V771V"
      },
      "source": [
        "## Temporal Activity Analysis\n",
        "\n",
        "Temporal aggregations are commonly used for monitoring transaction volume\n",
        "and detecting unusual spikes in activity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI5koJki8ANs",
        "outputId": "592865a3-9ddc-41b1-a681-032c33550f56"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"timestamp\",\n",
        "    expr(\"timestampadd(DAY, cast(rand()*365 as int), timestamp('2024-01-01'))\")\n",
        ")\n",
        "\n",
        "daily_volume = (\n",
        "    df.withColumn(\"date\", to_date(\"timestamp\"))\n",
        "      .groupBy(\"date\")\n",
        "      .agg(\n",
        "          count(\"*\").alias(\"num_transactions\"),\n",
        "          sum(\"amount\").alias(\"total_amount\")\n",
        "      )\n",
        "      .orderBy(\"date\")\n",
        ")\n",
        "\n",
        "daily_volume.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKpguvLw8YVX"
      },
      "source": [
        "## Persist Aggregated Outputs\n",
        "\n",
        "Aggregated datasets are persisted in Parquet format to simulate downstream\n",
        "consumption by analytics or monitoring systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-34uJPzM8Z9L"
      },
      "outputs": [],
      "source": [
        "user_metrics.write.mode(\"overwrite\").parquet(\"/content/output/user_metrics\")\n",
        "country_channel_stats.write.mode(\"overwrite\").parquet(\"/content/output/country_channel_stats\")\n",
        "daily_volume.write.mode(\"overwrite\").parquet(\"/content/output/daily_volume\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "144z5BEV8b9I"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This mini project demonstrates how PySpark can be used to generate and analyze\n",
        "large-scale transactional data using distributed processing.\n",
        "\n",
        "The workflow focuses on aggregation, segmentation and behavioral metrics that\n",
        "are commonly used in fintech and AML environments as a foundation for monitoring\n",
        "and risk assessment systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4duUcPa8dGL"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
